---
layout: post
title: "HomeWork 4"
date: 2024-10-24
markdown: kramdown
---

## Research Topics: Theory (T)

- **T1**:  Illustrate the concept of statistical independence, showing also the analogies with the formal definitions in probability theory.


## Research Topics: Applications (A)

- **A1**: Refine your stochastic SDE simulator to generate a continuous time, process to represent the scaling limit of the random Walk. To create the approximation of time continuity subdivide your reference temporal window into vanishing intervals dt and on each infinitesimal interval assign a probability p or p to make a jump of a + or - sqrt(dt). Note the significance of the simulation (Donsker invariance principle/ theorem or the functional central limit theorem) in relation to the Wiener process. 

# <span style="color:red">Researches about Theory (T)</span>

# Statistical Independence

Statistical independence is a fundamental concept in probability and statistics. It describes the relationship between two events where the occurrence of one event does not affect the probability of the other.

## 1. Formal Definition in Probability Theory

In probability theory, two events $\( A \)$ and $\( B \)$ are said to be **independent** if and only if:

$$
P(A \cap B) = P(A) \cdot P(B)
$$

This definition implies that the probability of both $\( A \)$ and $\( B \)$ happening together (their intersection) is equal to the product of their individual probabilities. 

- **Intuition**: If knowing that $\( A \)$ occurred provides no information about $\( B \)$'s likelihood (and vice versa), then $\( A \)$ and $\( B \)$ are independent.

### Example
If you flip a fair coin and roll a fair die, the outcomes of these two events are independent. Knowing the coin shows "Heads" doesn’t affect the likelihood that the die shows a "3". Formally:

$$
P(\text{Heads and 3}) = P(\text{Heads}) \cdot P(\text{3}) = \frac{1}{2} \times \frac{1}{6} = \frac{1}{12}
$$

## 2. Generalization to Random Variables

For two random variables $\( X \)$ and $\( Y \)$, independence means that the joint probability distribution factors as the product of the marginal distributions:

$$
P(X = x, Y = y) = P(X = x) \cdot P(Y = y)
$$

This can be extended to **conditional independence** and **mutual independence** among multiple variables.

### Conditional Independence
Two events $\( A \)$ and $\( B \)$ are conditionally independent given a third event $\( C \)$ if:

$$
P(A \cap B | C) = P(A | C) \cdot P(B | C)
$$

This means that given knowledge of $\( C \)$, the events $\( A \)$ and $\( B \)$ behave independently of each other.

## 3. Analogy with Independence in Real Life

Statistical independence can be compared to everyday situations where one event does not influence another:

- **Examples**: 
  - Weather and stock prices: In many cases, the chance of rain and fluctuations in a particular stock's price are statistically independent events.
  - Tossing coins: The result of one toss does not influence another; hence, each toss is independent.

## 4. Visual Representation

Imagine two separate circles representing the probability of two events, $\( A \)$ and $\( B \)$, on a Venn diagram. When these events are independent, there is no overlap (no “informational link”) affecting each other beyond their probability product.


## <span style="color:red">Researches about Applications (A)</span>


# Wiener Process (Brownian Motion) Approximation

This simulation approximates the **Wiener Process** (or **Brownian Motion**) as the scaling limit of a random walk, following the **Donsker Invariance Principle**. This principle demonstrates that a properly scaled random walk converges to a Wiener process as the number of steps approaches infinity and the time interval between steps approaches zero.

## Simulation Steps

### 1. Initialize Parameters
   - **Total Time (`T`)**: The total time duration of the simulation, set to 1 for simplicity.
   - **Number of Intervals (`n`)**: The number of steps in the simulation. Higher values increase precision.
   - **Time Step (`dt`)**: The time interval between each step, calculated as $\( T / n \)$.
   - **Step Size (`sqrtDt`)**: Each step has a random jump of size $\( \pm \sqrt{dt} \)$.
   - **Jump Probability (`p`)**: Probability of a jump in either direction; set to 0.5 for equal likelihood of + or - steps.

### 2. Generate the Wiener Process
   - For each step `i`:
     - Calculate a new random position `x` using a random choice of $\( +\sqrt{dt} \)$ or $\( -\sqrt{dt} \)$.
     - Accumulate `x` in an array to represent the Wiener process position over time.

### 3. Draw the Wiener Process
   - Convert time and position values to canvas coordinates.
   - Draw the process as a continuous line over time, starting at the origin.
   - Add axis labels:
     - **X-axis**: Represents time from 0 to $\( T \)$.
     - **Y-axis**: Represents the position of the Wiener process over time.

## Theoretical Significance

This simulation exemplifies the **Donsker Invariance Principle** (also called the **Functional Central Limit Theorem**). This principle states that as the number of steps increases (and `dt` decreases), the random walk path approaches a Wiener process path.

The simulation demonstrates:
   - **Random Walk Scaling Limit**: Each step's contribution scales as $\( \sqrt{dt} \)$, producing the continuous-time diffusion characteristic of Brownian motion.
   - **Wiener Process Approximation**: As the simulation runs, the random walk's behavior increasingly mirrors the smooth path of a Wiener process.

## Observations

- **Random Jumps**: Each step contributes a small movement, reflecting random fluctuations in either direction.
- **Smoothness of Path**: As `n` increases, the path appears more continuous, closely resembling the Wiener process.
