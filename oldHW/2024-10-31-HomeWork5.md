---
layout: post
title: "HomeWork 5"
date: 2024-10-31
markdown: kramdown
---

## Research Topics: Theory (T)

- **T1**: Prove in the simplest possible way the C-S (Cauchy-Schwarz) inequality (r coefficient normalizing denominator)

- **T2**: Reflect on the concepts of independence and uncorrelation, pointing out conceptual differences and possible measures.
 
[Optional Exercise: Regression Coefficients:
Derive the coefficients (b) and (a) of two regression lines using the least squares method, and show the relationships with R^2.] ðŸ‘‹


## Research Topics: Applications (A)

- E-M Simulator Enhancement:
Enhance your existing Euler-Maruyama (E-M) simulator by developing a unified simulation framework. Create a general central class that can possibly manage various types of stochastic differential equations (SDEs).

# Proof of the Cauchy-Schwarz Inequality

The **Cauchy-Schwarz inequality** in its simplest form can be written as:

$$\left( \sum_{i=1}^n x_i y_i \right)^2 \leq \left( \sum_{i=1}^n x_i^2 \right) \left( \sum_{i=1}^n y_i^2 \right)
$$

or in vector notation:

$$
|\mathbf{x} \cdot \mathbf{y}| \leq \|\mathbf{x}\| \|\mathbf{y}\|
$$

The goal is to show why this inequality holds.

## Step-by-Step Proof

1. **Construct a Function**:

   Define a function $\( f(t) \)$ using a parameter $\( t \)$:

   $$
   f(t) = \|\mathbf{x} - t \mathbf{y}\|^2
   $$

   Since it is a squared norm, $\( f(t) \geq 0 \)$.

2. **Expand $\( f(t) \)$**:

   Expand $\( f(t) \)$ in terms of $\( t \)$:

   $$
   f(t) = (\mathbf{x} - t \mathbf{y}) \cdot (\mathbf{x} - t \mathbf{y})
   $$

   Expanding the dot product gives:

   $$
   f(t) = \mathbf{x} \cdot \mathbf{x} - 2t (\mathbf{x} \cdot \mathbf{y}) + t^2 (\mathbf{y} \cdot \mathbf{y})
   $$

   Set $\( a = \mathbf{x} \cdot \mathbf{x} = \|\mathbf{x}\|^2 \)$, $\( b = \mathbf{y} \cdot \mathbf{y} = \|\mathbf{y}\|^2 \)$, and $\( c = \mathbf{x} \cdot \mathbf{y} \)$. Then,

   $$
   f(t) = a - 2ct + b t^2
   $$

3. **Find the Minimum of \( f(t) \)**:

   Since $\( f(t) \geq 0 \)$, its minimum value must also be non-negative. To find this minimum, treat $\( f(t) \)$ as a quadratic function in $\( t \)$ and set its derivative to zero:

   $$
   f'(t) = -2c + 2bt = 0
   $$

   Solving for $\( t \)$ gives:

   $$
   t = \frac{c}{b}
   $$

4. **Substitute $\( t = \frac{c}{b} \) into \( f(t) \)$**:

   Substitute $\( t = \frac{c}{b} \)$ into $\( f(t) \)$ to find the minimum value:

   $$
   f\left(\frac{c}{b}\right) = a - 2c \left(\frac{c}{b}\right) + b \left(\frac{c}{b}\right)^2
   $$

   Simplifying each term:

   $$
   f\left(\frac{c}{b}\right) = a - \frac{c^2}{b}
   $$

5. **Apply the Non-Negative Condition**:

   Since \( f(t) \geq 0 \), we have:

   $$
   a - \frac{c^2}{b} \geq 0
   $$

   Rearranging terms gives:

   $$
   c^2 \leq ab
   $$

6. **Rewrite in Terms of Norms**:

   Recall that $\( a = \|\mathbf{x}\|^2 \)$, $\( b = \|\mathbf{y}\|^2 \)$, and $\( c = \mathbf{x} \cdot \mathbf{y} \)$. Thus:

   $$
   (\mathbf{x} \cdot \mathbf{y})^2 \leq \|\mathbf{x}\|^2 \|\mathbf{y}\|^2
   $$

   Taking the square root of both sides, we arrive at the Cauchy-Schwarz inequality:

   $$
   |\mathbf{x} \cdot \mathbf{y}| \leq \|\mathbf{x}\| \|\mathbf{y}\|
   $$

This completes the proof.


# Reflection on Independence and Uncorrelation in Probability and Statistics

## 1. Independence

In probability theory, **independence** between two random variables $\( X \)$ and $\( Y \)$ means that knowing the outcome of one provides no information about the outcome of the other. Formally, $\( X \)$ and $\( Y \)$ are independent if for all values $\( x \)$ and $\( y \)$:

$$
P(X = x \text{ and } Y = y) = P(X = x) \cdot P(Y = y)
$$

### Key Characteristics of Independence:
- **Strong Condition**: Independence is a strict form of non-association, implying no predictable relationship between variables.
- **Joint Distribution Factorization**: The joint probability distribution of $\( X \)$ and $\( Y \)$ can be written as the product of their marginal distributions.
- **Non-Linear and Linear Relations**: Independence rules out both linear and nonlinear relationships between the variables.
  
### Measure of Independence:
- Independence is a binary condition and cannot be partially satisfied. **Mutual Information** is a commonly used measure to detect independence, as it quantifies the amount of information one variable provides about another.

## 2. Uncorrelation

In contrast, **uncorrelation** between two random variables $\( X \)$ and $\( Y \)$ specifically implies that their **linear relationship** is absent. Two variables are uncorrelated if their covariance is zero:

$$
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = 0
$$

Or equivalently, in terms of the correlation coefficient \( \rho \):

$$
\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} = 0
$$

### Key Characteristics of Uncorrelation:
- **Linear Independence Only**: Uncorrelation indicates no linear relationship but does not imply that other types of relationships (like quadratic or exponential) are absent.
- **Non-Binary**: Uncorrelation can occur in degrees; $\(\rho_{X,Y} \)$ ranges from -1 to 1, quantifying the degree of linear association between the variables.
  
### Measure of Uncorrelation:
- The **Pearson correlation coefficient** $\( \rho \)$ is the primary measure of uncorrelation. If $\( \rho = 0 \)$, the variables are uncorrelated.

## 3. Differences Between Independence and Uncorrelation

| Concept         | Independence                           | Uncorrelation                    |
|-----------------|---------------------------------------|----------------------------------|
| **Definition**  | No association at all                 | No linear association            |
| **Implication** | Implies uncorrelation                 | Does not imply independence      |
| **Conditions**  | Strict binary condition               | Can occur in degrees             |
| **Examples**    | Dice rolls, unrelated coin flips      | Orthogonal, nonlinear patterns   |

### Key Takeaways:
- **Independence is a stronger condition** than uncorrelation; all independent variables are uncorrelated, but uncorrelated variables are not necessarily independent.
- **Uncorrelated variables may still have a nonlinear relationship** (e.g., $\( X \)$ and $\( X^2 \)$ are uncorrelated but not independent).
- **Mutual information** and **correlation coefficient** serve as complementary measures in understanding the structure and dependencies in data.

## 4. Practical Examples:
1. **Independent Events**: Tossing two fair coins. Knowing one coin's result gives no information about the other.
2. **Uncorrelated but Dependent Variables**: For $\( X \sim U(-1, 1) \)$ and $\( Y = X^2 \)$, $\( X \)$ and $\( Y \)$ are uncorrelated since $\( E[(X - E[X])(Y - E[Y])] = 0 \)$ but not independent (knowing $\( X \)$ provides information about $\( Y \)$).

Independence and uncorrelation are foundational yet distinct concepts in probability theory and statistics, with important implications for modeling and interpreting real-world data.


## <span style="color:red">Researches about Applications (A)</span>

# Euler-Maruyama Simulator Enhancement for Stochastic Differential Equations (SDEs)

The goal of this enhancement is to create a unified, flexible simulation framework for solving various SDEs with the Euler-Maruyama method. By developing a central class to handle different types of SDEs, we gain a general, extensible solution.

## Overview of the Simulation Framework

This enhanced simulator will include:
1. **Core Class**: `SDESimulator` - A class to handle any SDE with custom drift and diffusion functions.
2. **Parameterization**: Flexibility to specify the drift, diffusion, time step, and time duration.
3. **Simulated Paths**: Option to generate multiple paths for the same SDE for visualization.
4. **General Interface**: To adjust parameters, choose the SDE type, and run simulations.

## SDE Basics

A stochastic differential equation (SDE) is generally written as:

$$
dX_t = \mu(X_t, t) \, dt + \sigma(X_t, t) \, dW_t
$$

where:
- $\( X_t \)$ is the process at time $\( t \)$,
- $\( \mu(X_t, t) \)$ is the **drift function**,
- $\( \sigma(X_t, t) \)$ is the **diffusion function**,
- $\( W_t \)$ is a Wiener process (Brownian motion).

The Euler-Maruyama method approximates this by:

$$
X_{t+ \Delta t} = X_t + \mu(X_t, t) \Delta t + \sigma(X_t, t) \sqrt{\Delta t} \cdot \epsilon
$$

where $\( \epsilon \)$ is a random sample from a standard normal distribution.

