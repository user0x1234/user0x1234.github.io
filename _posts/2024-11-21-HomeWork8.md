---
layout: post
title: "HomeWork 8"
date: 2024-11-21
markdown: kramdown
---

## Research Topics: Theory (T)

- **T1**: Recall the notion of Shannon Entropy amd other diversity measures of distributions

- **T2**: Recall the notion of primitive root (a primitive root modulo p a prime number is a number g such that for every
integer a that is coprime to p , there exists an integer k such that g^k \mod p = a )


## Research Topics: Applications (A)
- **A1**: Find and compile a sufficiently large piece of text by selecting several web pages and create a letter frequency distribution.
Choose a random shift value (e.g., 1-25, with wrap-around) and apply the Caesar cipher to encrypt the original text:
E = L + shift for each letter L of the message.
Use frequency analysis or find any efficient and effective strategy to find the shift and decrypt the message.

- **A2** Optional (Modular exponentiation)
Convert each letter of the original text to a numeric representation (A = 0, B = 1, ..., Z = 25).
Choose Parameters: Choose an exponent e and a modulus P. Ensure that e and P are coprime
(for example, you might choose ( e = 3 ) and ( P = 37)).
Calculate Encoded Values: Calculate the encoded values using the formula: E = L^e mod P
for each letter L of the message, where Lis the numeric representation of the letter.

See if you can find strategies and effective ways to get back the values of e and P. (In practice, certain values of e, like 3 or 65537 are commonly used. You may start with these values for e)

Visualize the distributions and calculate the Shannon entropy of the transformed distributions.
Summarize the findings from both parts of the exercise. Discuss how statistical analysis enhances understanding
of cryptographic algorithms and the importance of these skills in cybersecurity.


# <span style="color:red">Researches about Theory (T)</span>

# Shannon Entropy and Other Diversity Measures of Distributions

## 1. Shannon Entropy

Shannon Entropy is a measure of the uncertainty or unpredictability in a probability distribution. It quantifies the amount of "information content" or diversity in the distribution.  

The formula for Shannon Entropy $\( H \)$ is:

$$
H(X) = - \sum_{i} P(x_i) \log_b P(x_i)
$$

- $\( P(x_i) \)$: Probability of the $\(i\)$-th event $\(x_i\)$.  
- $\( b \)$: Base of the logarithm, commonly 2 (bits), $\(e\)$ (nats), or 10 (dits).  

### Properties:
1. $\( H(X) \geq 0 \)$: Entropy is non-negative.
2. Maximized for uniform distributions: The most uncertainty is present when all outcomes are equally likely.
3. Minimized when all the probability mass is concentrated on one event.

---

## 2. Other Diversity Measures

Beyond Shannon Entropy, there are several other measures to quantify diversity or uncertainty:

### 2.1. Rényi Entropy

Rényi Entropy generalizes Shannon Entropy with a parameter $\( \alpha \)$, allowing for a different emphasis on rare or common events:

$$
H_\alpha(X) = \frac{1}{1-\alpha} \log_b \left( \sum_{i} P(x_i)^\alpha \right)
$$

- $\( \alpha = 1 \)$: Reduces to Shannon Entropy.
- $\( \alpha > 1 \)$: Focuses more on high-probability events.
- $\( \alpha < 1 \)$: Focuses more on low-probability events.

---

### 2.2. Simpson's Diversity Index (or Herfindahl Index)

Used to measure the probability that two randomly chosen individuals belong to the same category:

$$
D = \sum_{i} P(x_i)^2
$$

- A smaller $\( D \)$ indicates higher diversity.
- Related to the **Inverse Simpson Index**, which is the reciprocal of $\( D \)$.

---

### 2.3. Gini-Simpson Index

A complement of the Simpson Index:

$$
G = 1 - D = 1 - \sum_{i} P(x_i)^2
$$

- Higher values indicate greater diversity.

---

### 2.4. Tsallis Entropy

Another generalization of Shannon Entropy:

$$
H_q(X) = \frac{1}{q-1} \left( 1 - \sum_{i} P(x_i)^q \right)
$$

- $\( q = 1 \)$: Reduces to Shannon Entropy.

---

### 2.5. Effective Number of Species

Converts diversity measures into an intuitive count of effective species:

$$
ENS = e^{H(X)}
$$

Where $\( H(X) \)$ is Shannon Entropy or another measure of entropy.

---

## 3. Applications

### Information Theory:
Shannon Entropy is foundational in coding theory, data compression, and communication systems.

### Ecology:
Measures like Shannon and Simpson Indexes are used to quantify biodiversity.

### Economics:
Herfindahl Index measures market concentration.

### Machine Learning:
Cross-entropy and KL divergence (based on Shannon Entropy) are used for classification problems.

---

## 4. Comparison of Diversity Measures

| Measure              | Emphasis                     | Typical Usage                               |
|----------------------|------------------------------|--------------------------------------------|
| Shannon Entropy      | General uncertainty          | Information theory, ecology, ML            |
| Rényi Entropy        | Tunable for rare events      | Security, robustness                       |
| Simpson Index        | Common events               | Biodiversity                               |
| Gini-Simpson Index   | Evenness of distribution     | Ecology, diversity analysis                |
| Tsallis Entropy      | Non-extensive systems        | Physics, complex systems                   |

Each measure emphasizes different aspects of the distribution's diversity, making them suited for different domains and purposes.


# **T1** Primitive Roots

## Definition

A **primitive root** modulo $\( p \)$, where $\( p \)$ is a prime number, is an integer $\( g \)$ such that every integer $\( a \)$ coprime to $\( p \)$ can be expressed in the form:

$\[
a \equiv g^k \mod p
\]$

for some integer $\( k \)$.

In other words:
- $\( g \)$ is called a **primitive root modulo $\( p \)**$ if the powers of $\( g \)$ (mod $\( p \)$) generate all the integers from 1 to $\( p-1 \)$ that are coprime to $\( p \)$.

---

## Key Properties

1. **Order**:
   - The smallest positive integer $\( k \)$ such that $\( g^k \equiv 1 \mod p \)$ is called the **order** of $\( g \)$.
   - $\( g \)$ is a primitive root if its order is $\( p-1 \)$ (the totient function $\( $\phi(p) = p-1 \)$ for prime $\( p \)$).

2. **Number of Primitive Roots**:
   - A prime $\( p \)$ has $\( \phi(p-1) \)$ primitive roots, where $\( \phi \)$ is Euler's totient function.

3. **Coprime Requirement**:
   - $\( g \)$ and $\( p \)$ must be coprime ($\(\gcd(g, p) = 1\)$).

4. **Modulo Exponentiation**:
   - For every integer $\( a \)$ coprime to $\( p \)$, there exists an integer $\( k \)$ such that:
    $$
     g^k \equiv a \mod p
     $$

---

## Examples

### Example 1: Primitive Roots Modulo 7
- $\( p = 7 \)$
- The integers coprime to 7 are: $\( \{1, 2, 3, 4, 5, 6\} \)$.
- Checking $\( g = 3 \)$:
  $$
  3^1 \mod 7 = 3,\quad 3^2 \mod 7 = 9 \mod 7 = 2,\quad 3^3 \mod 7 = 27 \mod 7 = 6,\quad 3^4 \mod 7 = 18 \mod 7 = 4,
  $$
  $$
  3^5 \mod 7 = 12 \mod 7 = 5,\quad 3^6 \mod 7 = 15 \mod 7 = 1
  $$
  - The sequence $\( \{3, 2, 6, 4, 5, 1\} \)$ contains all numbers coprime to 7.
  - Therefore, $\( g = 3 \)$ is a primitive root modulo 7.

---

### Example 2: Primitive Roots Modulo 9
- $\( p = 9 \)$ is **not a prime**, so primitive roots do not exist.

---

## Applications of Primitive Roots

1. **Cryptography**:
   - Used in Diffie-Hellman key exchange and discrete logarithm-based algorithms.
   
2. **Number Theory**:
   - Key in understanding cyclic groups and modular arithmetic.

3. **Random Number Generation**:
   - Primitive roots ensure uniform coverage of residues in a modulo system.

---

## Important Notes

- Not every integer $\( g \)$ modulo $\( p \)$ is a primitive root.
- Primitive roots exist only for:
  - Prime numbers $\( p \)$,
  - Powers of primes $\( p^k \)$ where $\( p > 2 \)$,
  - $\( 2, 4, p^k \cdot 2^m \)$ with restrictions.

---