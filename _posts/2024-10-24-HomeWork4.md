---
layout: post
title: "HomeWork 4"
date: 2024-10-24
markdown: kramdown
---

## Research Topics: Theory (T)

- **T1**:  Illustrate the concept of statistical independence, showing also the analogies with the formal definitions in probability theory.


## Research Topics: Applications (A)

- **A1**: Refine your stochastic SDE simulator to generate a continuous time, process to represent the scaling limit of the random Walk. To create the approximation of time continuity subdivide your reference temporal window into vanishing intervals dt and on each infinitesimal interval assign a probability p or p to make a jump of a + or - sqrt(dt). Note the significance of the simulation (Donsker invariance principle/ theorem or the functional central limit theorem) in relation to the Wiener process. 

# <span style="color:red">Researches about Theory (T)</span>

# Statistical Independence

Statistical independence is a fundamental concept in probability and statistics. It describes the relationship between two events where the occurrence of one event does not affect the probability of the other.

## 1. Formal Definition in Probability Theory

In probability theory, two events $\( A \)$ and $\( B \)$ are said to be **independent** if and only if:

$$
P(A \cap B) = P(A) \cdot P(B)
$$

This definition implies that the probability of both $\( A \)$ and $\( B \)$ happening together (their intersection) is equal to the product of their individual probabilities. 

- **Intuition**: If knowing that $\( A \)$ occurred provides no information about $\( B \)$'s likelihood (and vice versa), then $\( A \)$ and $\( B \)$ are independent.

### Example
If you flip a fair coin and roll a fair die, the outcomes of these two events are independent. Knowing the coin shows "Heads" doesn’t affect the likelihood that the die shows a "3". Formally:

$$
P(\text{Heads and 3}) = P(\text{Heads}) \cdot P(\text{3}) = \frac{1}{2} \times \frac{1}{6} = \frac{1}{12}
$$

## 2. Generalization to Random Variables

For two random variables $\( X \)$ and $\( Y \)$, independence means that the joint probability distribution factors as the product of the marginal distributions:

$$
P(X = x, Y = y) = P(X = x) \cdot P(Y = y)
$$

This can be extended to **conditional independence** and **mutual independence** among multiple variables.

### Conditional Independence
Two events $\( A \)$ and $\( B \)$ are conditionally independent given a third event $\( C \)$ if:

$$
P(A \cap B | C) = P(A | C) \cdot P(B | C)
$$

This means that given knowledge of $\( C \)$, the events $\( A \)$ and $\( B \)$ behave independently of each other.

## 3. Analogy with Independence in Real Life

Statistical independence can be compared to everyday situations where one event does not influence another:

- **Examples**: 
  - Weather and stock prices: In many cases, the chance of rain and fluctuations in a particular stock's price are statistically independent events.
  - Tossing coins: The result of one toss does not influence another; hence, each toss is independent.

## 4. Visual Representation

Imagine two separate circles representing the probability of two events, $\( A \)$ and $\( B \)$, on a Venn diagram. When these events are independent, there is no overlap (no “informational link”) affecting each other beyond their probability product.


## <span style="color:red">Researches about Applications (A)</span>

# Continuous-Time Attack Simulation Using Jumps