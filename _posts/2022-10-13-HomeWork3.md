---
layout: post
title: "HomeWork 3"
date: 2022-10-13
---


## Researches about pure theory (T)

T5. Illustrate the concept of conditional, joint, marginal (relative) frequency using a simple bivariate distribution<br />

T6. Illustrate the concept of statistical independence and the resulting mathematical relationships between the above frequencies

## Researches about applications (A)

A5. Create a distribution from the data obtained by the sniffer Wireshark by reading the CSV file or realtime data generated by the program<br />

[optional: create a bivariate distribution]

## Researches about theory relevant to applications (TA)

TA3. A survey on ONLINE algorithms (mean, variance, median, etc...)<br />

TA4. Illustrate in particular, Knuth recursion for the computation of the arithmetic mean or average, discussion why it is preferable to the "naive" algo



## <span style="color:red">Researches about theory (T)</span>


##  Concept of conditional, joint, marginal frequency 
Probability quantifies the uncertainty of the outcomes of a random variable.

It is relatively easy to understand and compute the probability for a single variable. Nevertheless, in machine learning, we often have many random variables that interact in often complex and unknown ways.

There are specific techniques that can be used to quantify the probability for multiple random variables, such as the joint, marginal, and conditional probability. These techniques provide the basis for a probabilistic understanding of fitting a predictive model to data.

Bivariate joint frequencies are displayed in the center of a frequency distribution table.<br/> 

![temperatureClass](/assets/HomeWork3/bivariateDistribution.PNG)


  →  Joint probability is the probability of two events occurring simultaneously.<br />
  →  Marginal probability is the probability of an event irrespective of the outcome of another variable.<br />
  →  Conditional probability is the probability of one event occurring in the presence of a second event.
<br />


##  Statistical Independence

Statistical independence is a concept in probability theory. Two events A and B are statistical independent if and only if their joint probability can be factorized into their marginal probabilities, i.e., P(A ∩  B) = P(A)P(B). <br/>
If two events A and B are statistical independent, then the conditional probability equals the marginal probability: P(A|B) = P(A) and P(B|A) = P(B). The concept can be generalized to more than two events. The events A<sub>1</sub>, …, A<sub>n</sub> are independent if and only if P(⋂<sup>n</sup><sub>i=1</sub>A<sub>i</sub>)=∏<sub>n</sub><sup>i=1</sup>P(A<sub>i</sub>).

## <span style="color:red"> Researches about applications (A)</span>


## <span style="color:red"> Researches about theory relevant to applications (TA)</span>

Algorithms for calculating variance play a major role in computational statistics. A key difficulty in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values. <br/>

Therefore, a naïve algorithm to calculate the estimated variance is given by the following:

    Let n ← 0, Sum ← 0, SumSq ← 0
    For each datum x:
        n ← n + 1
        Sum ← Sum + x
        SumSq ← SumSq + x × x
    Var = (SumSq − (Sum × Sum) / n) / (n − 1)


## KNUTH’S ALGORITHM

Donald Knuth in his “The Art Of Computer Programming” (1998) elaborate a numerically stable algorithm for the sample variance, also able to calculate the mean:

def online_variance(data):
    n = 0
    mean = M2 = 0.0

    for x in data:
        n += 1
        delta = x - mean
        mean += delta/n
        delta2 = x - mean
        M2 += delta*delta2

    if n < 2:
        return float('nan')
    else:
        return M2 / (n - 1)

This algorithm could have a loss in accuracy because of the division operation inside the loop.


[C# src](https://github.com/user0x1234/user0x1234.github.io/tree/main/src/HomeWork3/)
