---
layout: post
title: "HomeWork 3"
date: 2022-10-13
---


## Researches about pure theory (T)

T5. Illustrate the concept of conditional, joint, marginal (relative) frequency using a simple bivariate distribution<br />

T6. Illustrate the concept of statistical independence and the resulting mathematical relationships between the above frequencies

## Researches about applications (A)

A5. Create a distribution from the data obtained by the sniffer Wireshark by reading the CSV file or realtime data generated by the program<br />

[optional: create a bivariate distribution]

## Researches about theory relevant to applications (TA)

TA3. A survey on ONLINE algorithms (mean, variance, median, etc...)<br />

TA4. Illustrate in particular, Knuth recursion for the computation of the arithmetic mean or average, discussion why it is preferable to the "naive" algo



## <span style="color:red">Researches about theory (T)</span>

Probability quantifies the uncertainty of the outcomes of a random variable.

It is relatively easy to understand and compute the probability for a single variable. Nevertheless, in machine learning, we often have many random variables that interact in often complex and unknown ways.

There are specific techniques that can be used to quantify the probability for multiple random variables, such as the joint, marginal, and conditional probability. These techniques provide the basis for a probabilistic understanding of fitting a predictive model to data.

In this post, you will discover a gentle introduction to joint, marginal, and conditional probability for multiple random variables.


  →  Joint probability is the probability of two events occurring simultaneously.<br />
  →  Marginal probability is the probability of an event irrespective of the outcome of another variable.<br />
  →  Conditional probability is the probability of one event occurring in the presence of a second event.
<br />


##  Statistical Independence

Statistical independence is a concept in probability theory. Two events A and B are statistical independent if and only if their joint probability can be factorized into their marginal probabilities, i.e., P(A ∩ B) = P(A)P(B). If two events A and B are statistical independent, then the conditional probability equals the marginal probability: P(A|B) = P(A) and P(B|A) = P(B). The concept can be generalized to more than two events. The events A1, …, A n are independent if and only if P(⋂ni=1Ai)=∏ni=1P(Ai).

## <span style="color:red"> Researches about applications (A)</span>


## <span style="color:red"> Researches about theory relevant to applications (TA)</span>


[C# src](https://github.com/user0x1234/user0x1234.github.io/tree/main/src/HomeWork3/)
