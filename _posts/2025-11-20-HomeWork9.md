---
layout: post
title: "HomeWork 9"
date: 2025-11-13
markdown: kramdown
---

# 1. Interpretations of Probability

Probability has several classical interpretations that historically attempted to formalize uncertainty.

## 1.1 Classical Interpretation
Defines probability as:

$$
P(A) = \frac{\text{favorable outcomes}}{\text{equally likely outcomes}}
$$

Works well for games of chance, but fails when outcomes are not equally likely.

## 1.2 Frequentist Interpretation
Probability is the *limiting frequency* of an event:

$$
P(A)=\lim_{n\to\infty} \frac{N_n(A)}{n}
$$

Cannot assign probability to unique events.

## 1.3 Bayesian Interpretation
Probability is a "degree of belief", updated via Bayes' theorem:

$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

Handles unique events but depends on priors.

## 1.4 Geometric Interpretation
Defines probability using geometric ratios (length/area/volume), used in continuum problems.

---

# 2. Axiomatic Approach and Resolution of Conflicts

Kolmogorov introduced three axioms:

1. **Non-negativity:** $\(P(A) \ge 0\)$  
2. **Normalization:** $\(P(\Omega)=1\)$  
3. **Countable additivity:** If $\(A_i\)$ are disjoint,  
   $$
   P\left(\bigcup_i A_i\right) = \sum_i P(A_i)
   $$

This approach:

- does **not** assume equally likely outcomes,  
- unifies frequentist and Bayesian interpretations,  
- supports continuous and infinite sample spaces,  
- provides a consistent mathematical foundation.

---

# 3. Probability and Measure Theory

Probability theory is a special case of measure theory.

## 3.1 σ-Algebra
A σ-algebra $\( \mathcal{F} \)$ satisfies:

- $\( \Omega \in \mathcal{F} \)$
- closed under complements
- closed under countable unions

## 3.2 Probability Measure
A function  
$$
P: \mathcal{F} \to [0,1]
$$  
satisfying Kolmogorov’s axioms.

## 3.3 Random Variables
A random variable is a measurable function:

$$
X: $\Omega \to \mathbb{R}$
$$

such that $\( \{X \le x\} \in \mathcal{F} \)$.

---

# 4. Results Derived from the Axioms

## 4.1 Subadditivity

$$
P(A \cup B) \le P(A) + P(B)
$$

**Proof:**  
Write
$$
A \cup B = A \cup (B \setminus A)
$$
and use countable additivity.

## 4.2 Inclusion–Exclusion Principle

$$
P(A\cup B)=P(A)+P(B)-P(A\cap B)
$$

Generalizes to three or more sets.

---

# 5. Simulation of a Counting Process

We simulate events occurring randomly over $\([0,1]\)$ with average rate $\( \lambda \)$.

Divide time into $\( n \)$ intervals, each with event probability $\( p=\lambda/n \)$.

As $\( n\to\infty \)$, this approximates a **Poisson process**.

---

# 6. JavaScript Simulation (Interactive)

The following HTML+JS runs directly inside this post if your Jekyll theme allows HTML inside Markdown.


<div style="background:#1b2430; padding:15px; border-radius:8px; color:white;">
  <h2>Poisson Process Simulation</h2>

  λ: <input id="lambda" type="number" value="5" step="1"><br>
  n intervals: <input id="n" type="number" value="5000" step="500"><br><br>
  <button onclick="runSim()">Run Simulation</button>

  <canvas id="chart" width="600" height="300"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
function runSim() {
  const λ = parseFloat(document.getElementById("lambda").value);
  const n = parseInt(document.getElementById("n").value);
  const p = λ / n;

  let counts = 0;
  let t = [...Array(n).keys()].map(i => i/n);
  let events = [];

  for (let i=0; i<n; i++) {
    if (Math.random() < p) counts++;
    events.push(counts);
  }

  new Chart(document.getElementById("chart"), {
    type: "line",
    data: {
      labels: t,
      datasets: [{
        label: "Counting Process N(t)",
        data: events,
        borderWidth: 1,
        fill: false
      }]
    }
  });
}
</script>