---
layout: post
title: "HomeWork 2"
date: 2022-10-06
---

Homework 2
## Researches about theory (T)
1) Concept of distribution. Univariate and multivariate. Conditional and marginal distributions.

## Researches about applications (A)
1) make a simple program which uses the objects RANDOM and TIMER
2) Make a simple CSV parser
3) Compute an univariate distribution of a variable

## Researches about theory relevant to applications (TA)
CSV protocol RFC 4180 (definition and rules)


## <span style="color:red">Researches about theory (T)</span>	



In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.[1][2] It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).[3]
<br />
A probability distribution is a mathematical description of the probabilities of events, subsets of the sample space. The sample space, often denoted by Ω {\displaystyle \Omega } \Omega , is the set of all possible outcomes of a random phenomenon being observed; it may be any set: a set of real numbers, a set of vectors, a set of arbitrary non-numerical values, etc. For example, the sample space of a coin flip would be Ω = {heads, tails}. 

<br />
In statistics, a univariate distribution is a probability distribution of only one random variable. This is in contrast to a multivariate distribution, the probability distribution of a random vector (consisting of multiple random variables). 
<br />
## Univariate and multivariate distribution
One of the simplest examples of a discrete univariate distribution is the discrete uniform distribution, where all elements of a finite set are equally likely. It is the probability model for the outcomes of tossing a fair coin, rolling a fair die, etc. The univariate continuous uniform distribution on an interval [a, b] has the property that all sub-intervals of the same length are equally likely. 
<br/>

Given two random variables that are defined on the same probability space,[1] the joint probability distribution is the corresponding probability distribution on all possible pairs of outputs. The joint distribution can just as well be considered for any given number of random variables. The joint distribution encodes the marginal distributions, i.e. the distributions of each of the individual random variables. It also encodes the conditional probability distributions, which deal with how the outputs of one random variable are distributed when given information on the outputs of the other random variable(s). 

<br />
 ## What Are Marginal and Conditional Distributions?

In statistics, a probability distribution is a mathematical generalization of a function that describes the likelihood for an event to occur. Mathematically, the events are assigned to random variables, and the distribution can be used to calculate the likelihood that such a variable lies in a given range. Probability distributions are often represented in their mathematical form, p(x)
, where the variable x

represents the set of all possible outcomes or events.

Among the probability distributions that can be computed regarding random variables are both marginal and conditional distributions. A definition that will be useful to understand the concepts of marginal and conditional probability is that of joint probability. The joint probability is a distribution that represents the likelihood of two events occurring simultaneously. The joint probability of two possible events can be described with a distribution of the form p(x,y)
. This concept can be generalized to the joint probability of K arbitrary events p(z1,…,zK)

.

The marginal probability of an event is the probability distribution that describes only the subset of the event of interest, that is, a reduction of a general joint probability distribution so that it depends on a single event. For simplicity, when considering the case of a joint probability distribution that depends on two possible events, x
and y the marginal probability would correspond in this case to a function px(x), where the variable related to the event y

has been integrated out.

The conditional probability, on the other hand, is a distribution that represents the likelihood of an event to occur given a particular outcome of another event. It is usually represented as

p(x|y)

A particularly useful result that relates the joint probability of two variables with the conditional probability is Bayes' theorem:<br />

p(x|y)=p(x,y)p(y)

## Difference Between Marginal and Conditional Distributions

There are a few differences between the marginal and conditional distributions. To begin with, they describe different likelihoods. The marginal distribution describes the likelihood of an event to occur, independent of others, while the conditional distribution describes the likelihood of an event to occur given the outcome of another. As such, the marginal distribution is a function of only one variable, while the conditional distribution is a function of at least two, the event of interest and the event, or events, it is conditional on. 