---
layout: post
title: "HomeWork 2"
date: 2024-10-10
markdown: kramdown
---

## Research Topics: Theory (T)

- **T1**: Find the simplest and most elegant way to show the Welford recursion.
(Winner will have prize ðŸ˜Š )



## Research Topics: Applications (A)

- **A1**: Refine you Eulerâ€“Maruyama simulator to approximate numerical solution of a stochastic 
differential equation (SDE), buy adding the following variants to the existing framework:

A. Jumps -1 +1 with prob. p [random walk]
B. Absolute and relative frequency trajectories
C. Final distribution and intermediate distributions (at one internal
time/step selectable from the gui), with mean and variance.

(make it all parametric so that one unique interface will handle it all)


## Research Topics: Researches about theory relevant to applications (TA)

- **TA1**
Make your personal notes about the behavior of mean and variance wrt to time.
What did you observe in all the 4 different cases (relative/abs freq & Bernoulli/random walk) ?

# <span style="color:red">Researches about Theory (T)</span>

# (T1) Find the simplest and most elegant way to show the Welford recursion

## Welford's Algorithm for Mean and Variance

Welfordâ€™s algorithm provides an efficient way to compute the **mean** and **variance** in a single pass over the data, without the need to store all data points. This is particularly useful for large datasets or streaming data.

## Recursive Formulation

Let:
- $( x_1, x_2, \dots, x_n \)$ be the data points.
- $\( \mu_n \)$ be the mean after observing $\( n \)$ data points.
- $\( \sigma^2_n \)$ be the variance after observing $\( n \)$ data points.

### 1. Mean Recursion

The mean $\( \mu_n \)$ after $\( n \)$ data points is updated as:

$$
\mu_n = \mu_{n-1} + \frac{(x_n - \mu_{n-1})}{n}
$$

Where:
- $\( \mu_{n-1} \)$ is the mean after $\( n-1 \)$ data points.
- $\( x_n \)$ is the new data point.

### 2. Variance Recursion

The variance $\( \sigma^2_n \)$ can be updated using the intermediate sum of squares of differences from the mean:

$$
M_n = M_{n-1} + (x_n - \mu_{n-1})(x_n - \mu_n)
$$

Finally, the variance $\( \sigma^2_n \)$ is calculated as:

$$
\sigma^2_n = \frac{M_n}{n}
$$

### Algorithm Steps

1. **Initialization**:
   - Set initial values: 
     $$
     \mu_0 = 0, \quad M_0 = 0
     $$
   
2. **For each new data point \( x_n \)**:
   - Update the mean using:
     $$
     \mu_n = \mu_{n-1} + \frac{(x_n - \mu_{n-1})}{n}
     $$
   - Update the intermediate sum \( M_n \) using:
     $$
     M_n = M_{n-1} + (x_n - \mu_{n-1})(x_n - \mu_n)
     $$
   - Compute the variance using:
     $$
     \sigma^2_n = \frac{M_n}{n}
     $$

### Benefits of Welfordâ€™s Algorithm

- **Single-pass**: The algorithm computes the mean and variance in a single pass through the data, making it memory-efficient.
- **Numerical stability**: It avoids the precision issues associated with calculating variance using the naive two-pass approach.

This recursive method efficiently computes the mean and variance in real-time without requiring all data points to be stored in memory.



## <span style="color:red">Researches about Applications (A)</span>

# (A1) Euler-Maruyama Simulation with Random Walk for SDEs

This simulator approximates the numerical solution of a Stochastic Differential Equation (SDE) using the Eulerâ€“Maruyama method. It introduces random jumps in the system and tracks absolute and relative frequencies of states.

### Key Features:

1. **Jumps with Probability**:
   - The system can jump by $\(-1\)$ or $\(+1\)$ at each time step with probability $\(p\)$.
   - You can modify the probability $\(p\)$ to control the behavior of the random walk.

2. **Trajectory Plot**:
   - The blue line shows the trajectory of the system over time.
   - The trajectory moves up or down based on the random jump decisions at each step.

3. **Frequency Histogram**:
   - After the simulation, the green bars show how often each state is visited by the system (absolute frequency).
   - The number above each bar shows the count of visits to that state.

4. **User Controls**:
   - **Number of Time Steps**: Control how many time steps the simulation runs.
   - **Probability of Jump**: Set the probability $\(p\)$ for the random jumps.
   - **Step Size (Î”t)**: Define the step size for the simulation.

5. **Final Distribution**:
   - The simulator also calculates the final distribution of states, allowing you to see which values are most frequently visited.

[JS src](https://github.com/user0x1234/user0x1234.github.io/tree/main/src/hw2/js/)

## <span style="color:red">Researches about theory relevant to applications (TA)</span>

# Personal Notes on Mean and Variance Behavior over Time

### Key Observations:

1. **Bernoulli Process with Absolute Frequency**:
   - **Mean**: Drifts up or down depending on probability $\(p\)$ (if $\(p > 0.5\)$, mean increases).
   - **Variance**: Increases steadily over time due to greater deviation from the starting point.
   - **Distribution**: Broader and wider with time, with peaks at frequently visited states but more spread due to higher variance.

2. **Bernoulli Process with Relative Frequency**:
   - **Mean**: Stabilizes around 0.5 as the process tends toward equilibrium (for $\(p = 0.5\)$).
   - **Variance**: Decreases over time, reflecting a smoothing effect as more steps are taken.
   - **Distribution**: Narrows over time, reflecting convergence of relative frequencies toward the theoretical mean.

3. **Random Walk with Absolute Frequency**:
   - **Mean**: Shows significant drift influenced by initial conditions and parameters (step size, time increment).
   - **Variance**: Grows rapidly with time, leading to a highly spread-out final distribution.
   - **Distribution**: Widens significantly as time progresses due to the increasing randomness in state changes.

4. **Random Walk with Relative Frequency**:
   - **Mean**: Averages out over time, approaching a stable value due to normalization by step count.
   - **Variance**: Decreases as the number of steps grows, reducing the effect of early random fluctuations.
   - **Distribution**: Narrows gradually, similar to the Bernoulli process with relative frequency.

### Key Differences:
- **Absolute vs. Relative Frequency**: 
  - Absolute frequency measures the total number of successes, leading to broader distributions and larger variance.
  - Relative frequency normalizes the outcomes, resulting in more stable means and reduced variance over time.




